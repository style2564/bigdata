{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2065403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e67494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载和准备数据\n",
    "data = pd.read_csv('earthquake.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time\n",
    "data['Timestamp'] = data.apply(lambda row: pd.Timestamp(f\"{row['Date']} {row['Time']}\"), axis=1)\n",
    "data['Timestamp'] = data['Timestamp'].view('int64') // 10**9 #Unix时间戳\n",
    "features = data[['Timestamp', 'Longitude', 'Latitude']]\n",
    "targets = data[['Depth', 'Magnitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f798d71-1fa5-4e84-8fb4-bf15b30413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np=features.values\n",
    "y_np=targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f55c73b-a92a-4194-9944-f4e766459d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_x.fit(X_np)\n",
    "scaler_y.fit(y_np)\n",
    "\n",
    "# 使用相同的参数对训练数据和测试数据进行归一化\n",
    "X_normalized = scaler_x.transform(X_np)\n",
    "y_normalized = scaler_y.transform(y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6136a67-3a5b-415d-8d10-4a942d6e9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X, Y, window_size):\n",
    "    X_windows, Y_windows = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        # 从X中提取窗口\n",
    "        X_window = X[i:i+window_size]\n",
    "        # 从Y中提取紧接着窗口的下一个值作为标签\n",
    "        Y_window = Y[i+window_size]\n",
    "        X_windows.append(X_window)\n",
    "        Y_windows.append(Y_window)\n",
    "    return np.array(X_windows), np.array(Y_windows)\n",
    "\n",
    "window_size = 10\n",
    "X_windows, Y_windows = create_sliding_windows(X_normalized, y_normalized, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3a0dac-015d-4c36-a55d-a105c5e833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X_windows, dtype=torch.float32)\n",
    "Y = torch.tensor(Y_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7759d3-cebf-4095-a004-6e3cf248b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定测试集的大小\n",
    "test_size = 0.2\n",
    "\n",
    "# 计算测试集应有的数据点数量\n",
    "num_data_points = len(X)\n",
    "num_test_points = int(num_data_points * test_size)\n",
    "\n",
    "# 计算测试集和训练集的分割点\n",
    "split_point = num_data_points - num_test_points\n",
    "\n",
    "# 按顺序划分数据为训练集和测试集\n",
    "X_train = X[:split_point]\n",
    "X_test = X[split_point:]\n",
    "y_train = Y[:split_point]\n",
    "y_test = Y[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671d7c6c-7c06-4414-889d-5c27345f68c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18722, 10, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98436c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e47110-907c-4e08-8525-39ff6156453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ed2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_prob):\n",
    "        super(ComplexLSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        \n",
    "        # 添加Dropout层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # 定义额外的全连接层\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)  # 批量归一化层\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # 前向传播LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 只需要LSTM最后一层的输出\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 通过全连接层\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = 3 # 维度、经度、时间\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 2 # 震深和震级\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = ComplexLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout_prob)\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 将模型移到GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3292658c-78fb-45a4-a12f-93ce846ac6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 设置训练轮数\n",
    "num_epochs = 200\n",
    "\n",
    "# 用于保存最佳模型的逻辑\n",
    "best_loss = np.inf\n",
    "best_model_path = 'best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77d559d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.2044\n",
      "Epoch [20/200], Loss: 0.1334\n",
      "Epoch [30/200], Loss: 0.1531\n",
      "Epoch [40/200], Loss: 0.1134\n",
      "Epoch [50/200], Loss: 0.1439\n",
      "Epoch [60/200], Loss: 0.1666\n",
      "Epoch [70/200], Loss: 0.1461\n",
      "Epoch [80/200], Loss: 0.0978\n",
      "Epoch [90/200], Loss: 0.1424\n",
      "Epoch [100/200], Loss: 0.1326\n",
      "Epoch [110/200], Loss: 0.1191\n",
      "Epoch [120/200], Loss: 0.1410\n",
      "Epoch [130/200], Loss: 0.1639\n",
      "Epoch [140/200], Loss: 0.1486\n",
      "Epoch [150/200], Loss: 0.1290\n",
      "Epoch [160/200], Loss: 0.1413\n",
      "Epoch [170/200], Loss: 0.1180\n",
      "Epoch [180/200], Loss: 0.1052\n",
      "Epoch [190/200], Loss: 0.1470\n",
      "Epoch [200/200], Loss: 0.1385\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # 确保模型处于训练模式\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # 移动数据到GPU\n",
    "        optimizer.zero_grad() # 清除过往梯度\n",
    "        \n",
    "        outputs = model(inputs) # 前向传播\n",
    "        \n",
    "        loss = criterion(outputs, targets) # 计算损失\n",
    "        loss.backward() # 后向传播，计算梯度\n",
    "        optimizer.step() # 更新权重\n",
    "    \n",
    "    # 打印训练进度\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5258085-9dfd-4e5e-a842-a738e9e47aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestLoss: 0.0915\n"
     ]
    }
   ],
   "source": [
    "print(f'BestLoss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e35aad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval() # 将模型设置为评估模式\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad(): # 不计算梯度\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # 移动数据到GPU\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(targets.cpu().numpy())\n",
    "    \n",
    "    predictions = np.vstack(predictions)\n",
    "    actuals = np.vstack(actuals)\n",
    "    \n",
    "    # 反标准化以比较实际值\n",
    "    predictions = scaler_y.inverse_transform(predictions)\n",
    "    actuals = scaler_y.inverse_transform(actuals)\n",
    "    \n",
    "    # 计算均方根误差\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # 计算平均绝对误差\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690d601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 11.746\n",
      "Mean Absolute Error (MAE): 8.647\n"
     ]
    }
   ],
   "source": [
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)  # 确保模型在正确的设备上\n",
    "\n",
    "# 评估模型\n",
    "rmse, mae = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b013d-6951-4063-a149-7710f8a936f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f47247-56d3-4749-a6fe-ace671bd8efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897847c4-4b4e-4a8a-9b0d-20adcf88a5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe63874-6088-4a80-a943-902ca3031b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc8127a2-0869-4434-a60d-4d9c822bfbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 274049166/274049166 [12:40:32<00:00, 6005.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      Time  Latitude  Longitude        Type  Depth  Magnitude  \\\n",
      "0 1965-01-02  13:44:18    19.246    145.616  Earthquake  131.6        6.0   \n",
      "1 1965-01-04  11:29:49     1.863    127.352  Earthquake   80.0        5.8   \n",
      "2 1965-01-05  18:05:58   -20.579   -173.972  Earthquake   20.0        6.2   \n",
      "3 1965-01-08  18:49:43   -59.076    -23.557  Earthquake   15.0        5.8   \n",
      "4 1965-01-09  13:32:50    11.938    126.427  Earthquake   15.0        5.8   \n",
      "\n",
      "   Timestamp  Cluster  \n",
      "0 -157630542       -1  \n",
      "1 -157465811        1  \n",
      "2 -157355642        0  \n",
      "3 -157093817       -1  \n",
      "4 -157026430       -1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义Haversine距离的计算函数\n",
    "def haversine_distance(point1, point2):\n",
    "    return geodesic(point1, point2).kilometers\n",
    "\n",
    "# 将经纬度数据转换为DBSCAN需要的球面距离矩阵\n",
    "def create_distance_matrix(coordinates):\n",
    "    num_points = len(coordinates)\n",
    "    distance_matrix = np.zeros((num_points, num_points))\n",
    "    \n",
    "    # 创建一个tqdm进度条，total参数设置为距离矩阵的元素总数\n",
    "    # 由于距离矩阵是对称的，我们只需要计算上三角部分（不包括对角线）\n",
    "    num_elements = (num_points * (num_points - 1)) // 2\n",
    "    pbar = tqdm(total=num_elements)\n",
    "    \n",
    "    for i, point1 in enumerate(coordinates):\n",
    "        for j, point2 in enumerate(coordinates):\n",
    "            if i < j:  # 只计算上三角部分\n",
    "                distance_matrix[i][j] = haversine_distance(point1, point2)\n",
    "                distance_matrix[j][i] = distance_matrix[i][j]  # 对称复制到下三角部分\n",
    "                pbar.update(1)  # 更新进度条\n",
    "        pbar.refresh()  # 刷新进度条显示\n",
    "    \n",
    "    pbar.close()  # 完成时关闭进度条\n",
    "    return distance_matrix\n",
    "\n",
    "# 使用DBSCAN进行聚类\n",
    "def dbscan_clustering(coordinates, eps, min_samples):\n",
    "    distance_matrix = create_distance_matrix(coordinates)\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    dbscan.fit(distance_matrix)\n",
    "    return dbscan.labels_\n",
    "\n",
    "# 加载CSV数据\n",
    "df = data\n",
    "\n",
    "# 准备经纬度数据\n",
    "coordinates = df[['Latitude', 'Longitude']].values\n",
    "\n",
    "# 进行聚类，这里 eps 和 min_samples 需要根据数据调整\n",
    "eps_in_kilometers = 1000  # 聚类半径100公里\n",
    "min_samples = 2000          # 一个聚类中最小的样本数量\n",
    "labels = dbscan_clustering(coordinates, eps_in_kilometers, min_samples)\n",
    "\n",
    "# 将聚类标签添加到原始数据中\n",
    "df['Cluster'] = labels\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eb93389-b20a-4bd8-a6aa-30cd317136c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('process_earthquake.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
